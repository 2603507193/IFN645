{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Decision Tree Mining\n",
    "\n",
    "### In this practical\n",
    "1. [Resuming from practical 2](#resume)\n",
    "2. [Building your first decision tree model](#build)\n",
    "3. [Understanding and visualising your decision tree](#viz)\n",
    "4. [Finding optimal hyperparameters with GridSearchCV](#gridsearch)\n",
    "\n",
    "---\n",
    "\n",
    "### Important Changelog:\n",
    "* (25/07/2017) Made tutorial notes public.\n",
    "* (09/08/2017) Added explanation on classification report metrics and DT visualisation.\n",
    "* (14/08/2017) Updated explanation for some parts.\n",
    "\n",
    "This practical note introduces predictive modelling using decision tree in Python. Decision trees are relatively simple models, yet they can be powerful and accurate if built and utilised properly. These models will help to classify the lapsing donors based on their responses to greeting card mailing conducted by the national veteran organisation.\n",
    "\n",
    "Predictive modelling, including decision trees, start with a training data set. Observations in a training data set are known as *training cases/samples/rows/instances/records*. The variables are called *inputs* (or *variables/attributes/columns/features/explanatory variables or independent variables*) and *targets* (or *response/outcome/output/class/dependent variables*). For a given case, input variables are used to estimate target variable.\n",
    "\n",
    "As you learned in practical 2, measurement type/role for inputs and target can be varied. In this case, we are building predictive models to predict TARGETB, which is a binary variable.\n",
    "\n",
    "**This tutorial notes is in experimental version. Please give us feedbacks and suggestions on how to make it better. Ask your tutor for any question and clarification.**\n",
    "\n",
    "## 1. Resuming from practical 2 <a name=\"resume\"></a>\n",
    "Last practical, we learned how to perform data preparation on our dataset. We build plots of data distribution, dealt with noisy values, imputed missing values and drop columns not required in our analysis. After practical 2, your code should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside dm_tools.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_prep():\n",
    "    # read the pva97nk dataset\n",
    "    df = pd.read_csv('pva97nk.csv')\n",
    "    \n",
    "    # change DemCluster from interval/integer to nominal/str\n",
    "    df['DemCluster'] = df['DemCluster'].astype(str)\n",
    "    \n",
    "    # change DemHomeOwner into binary 0/1 variable\n",
    "    dem_home_owner_map = {'U':0, 'H': 1}\n",
    "    df['DemHomeOwner'] = df['DemHomeOwner'].map(dem_home_owner_map)\n",
    "    \n",
    "    # denote errorneous values in DemMidIncome\n",
    "    mask = df['DemMedIncome'] < 1\n",
    "    df.loc[mask, 'DemMedIncome'] = np.nan\n",
    "    \n",
    "    # impute missing values in DemAge with its mean\n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "\n",
    "    # impute med income using mean\n",
    "    df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
    "\n",
    "    # impute gift avg card 36 using mean\n",
    "    df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n",
    "    \n",
    "    # drop ID and the unused target variable\n",
    "    df.drop(['ID', 'TargetD'], axis=1, inplace=True)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in our project space / iPython console\n",
    "from dm_tools import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building your first decision tree <a name=\"build\"></a>\n",
    "\n",
    "### 2.1. Data partitioning\n",
    "\n",
    "Before building a decision tree, we need to set up data partitions. In a data mining project, a dataset is typically split into training, validation and test set.\n",
    "* **Training dataset** is a set of examples that is used to build the model. Most training algorithms tends to **overfit** the training dataset, which means the model learns of empirical relationships specific to the provided data which might not be true in general. Overfitting model tends to perform horribly outside of the training set. We will see the example of overfitting soon.\n",
    "* **Validation dataset** is a set of examples that is used to evaluate the performance of a model. Validation dataset is unseen (not used in training/fitting process) and typically has similar distribution with training dataset. Validation dataset is commonly used to estimate performance and choose one of a number of different models.\n",
    "* **Test dataset** is a set of examples used to estimate the performance of a model in practice. Similar to validation, test dataset is unseen. However, it is not used in model selection process.\n",
    "\n",
    "In `sklearn`, splitting a dataset into train and test is commonly done using `train_test_split` function from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function is imported, we can start partitioning the dataset. The convention in Python is to assign input variables as `X` and target as `y`. In our case, `y` would be `TargetB` and `X` would be the rest of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# target/input split\n",
    "y = df['TargetB']\n",
    "X = df.drop(['TargetB'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert `X` (which is still a pandas `DataFrame` object) into a `numpy` matrix that can be consumed by `sklearn`. This step can be done using `.as_matrix()` function from pandas DataFrame.\n",
    "\n",
    "After that, use the `train_test_split` function to split them into 50% training and 50% test data. Typically, this configuration is sufficient to train on. However, if the class distribution is uneven (e.g. if in our data set there are 70% non-donors to 30% repeat donors), we want to ensure there is enough representation of the minority class in the training set. In this case, we need larger training set, such as 70/30 or 66/33.\n",
    "\n",
    "In performing the split, we also need to stratify based on `y`. Stratification ensures the same ratio of positive and negative targets in both train and test data set.\n",
    "\n",
    "As `train_test_split` shuffles the dataset before splitting it, it is important to set a consistent random state, which is the seed number used to generate the shuffle. We will use random seed of 42 in all practicals in this unit. In practice, you can use any integer number as long as it is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Fitting your model\n",
    "We are ready to build our model. Let's start by importing `DecisionTreeClassifier`, initialise a model, and then training it using `.fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just trained your first model. Congratulations! :D\n",
    "\n",
    "Take a closer look on the output of the model fit. It prints out the key model hyperparameters. Some of the crucial ones are\n",
    "1. **Criterion:** Method to evaluate the quality of a split. This model uses **gini** method.\n",
    "2. **Max depth:** The maximum depth of the tree. Deeper models are more complex and have more nodes. This model has no depth limitation, which means it can fit the data really well (a bit too well sometimes).\n",
    "3. **Min samples leaf:** The minimum number of samples required to be at a leaf node, allowing us to limit the minimum size of a leaf node. This model has min samples leaf of 1, almost no limitation of node leaf.\n",
    "\n",
    "Please note of these hyperparameters. We will discuss the importance of them and how to tune them to produce better model.\n",
    "\n",
    "Once the model is trained, it is important to assess its performance. A very common way to check the quality of a predictive classification model is `accuracy`. We can use `.score` to find out its accuracy against a specific set of data. Start by scoring the model againsts the data the model is trained (testing how well the model fits the training data).\n",
    "\n",
    "> **Accuracy**: *Accuracy of a predictive classification model refers to the ability of the model to predict the labels of a dataset correctly. It is calculated by dividing the number of **correct predictions** by the number of **data points*** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model has managed to learn all of the patterns in training data and is able to predict with 100% accuracy. Does it mean this model works perfectly? Not necessarily. We need to check whether it can replicate the performance on similar, unseen data i.e. the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.514857679074\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model performs really well on the training dataset, it actually fails to accurately predict data in the test set. This is a clear indication of overfitting on training. Therefore, we need to tune the hyperparameters of our model and ensure it can generalise better to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tips: As an alternative to accuracy, we could also use `classification_report` to assess performance of a classifier. Classification report outputs a number of statistics for each target class:\n",
    "> 1. Precision: Proportion of all positive predictions that are correct. Precision is a measure of how many positive > predictions were actual positive observations.\n",
    "> 2. Recall: Proportion of all real positive observations that are correct. Precision is a measure of how many actual positive observations were predicted correctly.\n",
    "> 3. F1: The harmonic mean of precision and recall. F1 score is an 'average' of both precision and recall.\n",
    "> 4. Support: Number of instances in each class.\n",
    "\n",
    "> Read more: https://chrisalbon.com/machine-learning/precision_recall_and_F1_scores.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.53      0.52      1599\n",
      "          1       0.52      0.50      0.51      1598\n",
      "\n",
      "avg / total       0.51      0.51      0.51      3197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding and visualising your decision tree\n",
    "\n",
    "### 3.1. Feature importance\n",
    "\n",
    "Let's take a deeper look on the decision tree that we just built. Firstly, it is important to get insights of which features/input variables that are impactful to the decision making process in our model. This is commonly known as **feature importance**. In an `sklearn` decision tree, feature importance is stored within the model itself in form of **(feature_index, feature_importance_value)**. For easier interpretation, let's match this data with feature names from `X`, sort them in descending order of importance and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DemMedHomeValue : 0.0969972956867\n",
      "GiftAvgAll : 0.0767356306444\n",
      "DemAge : 0.0763231909358\n",
      "DemPctVeterans : 0.0663784806783\n",
      "DemMedIncome : 0.0585577800646\n",
      "GiftTimeFirst : 0.0436003526735\n",
      "GiftAvgCard36 : 0.0411936154952\n",
      "GiftAvgLast : 0.0394245652533\n",
      "PromCntAll : 0.0378038342202\n",
      "GiftAvg36 : 0.0377843722931\n",
      "GiftCntAll : 0.0372488795585\n",
      "PromCnt36 : 0.036758442239\n",
      "GiftCntCardAll : 0.0309206970042\n",
      "GiftTimeLast : 0.0294552730442\n",
      "PromCnt12 : 0.0269378675816\n",
      "PromCntCardAll : 0.0254693333384\n",
      "GiftCnt36 : 0.0240645351531\n",
      "PromCntCard36 : 0.0228536606905\n",
      "GiftCntCard36 : 0.0149248534475\n",
      "PromCntCard12 : 0.0106353549802\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In descending order, the top 3 important variables for this model are `DemMedHomeValue`, `DemIncome` and `DemAge`. Feature importance is really important to not only understand the model, but also to learn more about the data and present conclusions to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Visualising decision tree structure\n",
    "\n",
    "Other than feature importance, we could also gain insights of our decision tree by visualising it. To do this, use the `export_graphviz` function and `pydot` module. Open the `.png` file to view the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will take a long time and your picture will be very large and incomprehensible. This shows that the model is very complex and deep, which is a typical characteristic of an overfitting model.\n",
    "\n",
    "Let's limit the complexity of the model by setting the `max_depth` that the model can go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.579904453691\n",
      "Test accuracy: 0.55458242102\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.71      0.61      1599\n",
      "          1       0.58      0.40      0.47      1598\n",
      "\n",
      "avg / total       0.56      0.55      0.54      3197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#retrain with a small max_depth limit\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could see that the simpler model (small `max_depth`) actually performs much better on the test data. `max_depth` on a decision tree is what we call a hyperparameter (or just parameter) and they are responsible for the structure of a model. Different combinations of parameters will produce different models with different performance too.\n",
    "\n",
    "Let's do a feature importance analysis and visualization on this new decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.497565606409\n",
      "DemMedHomeValue : 0.213818194825\n",
      "GiftAvgCard36 : 0.151128586153\n",
      "GiftTimeLast : 0.100882125056\n",
      "GiftCntCard36 : 0.036605487557\n",
      "DemGender_U : 0.0\n",
      "DemCluster_11 : 0.0\n",
      "StatusCat96NK_L : 0.0\n",
      "StatusCat96NK_N : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "DemCluster_12 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "StatusCat96NK_E : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_16 : 0.0\n",
      "DemCluster_17 : 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=X.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"week3_dt_viz.png\") # saved in the following file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple decision tree structure](http://dataminingtuts.s3.amazonaws.com/week3_dt_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree starts splitting with `GiftAvgLast`, with `GiftAvgCard36` and `GiftTimeLast` as the competing splits. We have around 8 leaf nodes here, and you can see the number of samples and value splits in each node (e.g. for the left most leaf, we have 757 customers here, with distribution of 262 non-donors to 495 donors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameters with GridSearchCV\n",
    "\n",
    "Coming back to the question of producing a model that does not overfit. How do we find the optimal combination of hyperparameters for a type of model on a given dataset?\n",
    "\n",
    "A common method to find this optimal combination is to run a exhaustive search over a subset of possible values of hyperparameters to optimise, commonly known as grid search. Assume we would like to find the optimal `max_depth` and `min_samples_leaf` for our decision tree. We would like to try maximum of [5, 10, 20] depth and [10, 20, 30] minimum samples in a leaf. Grid search creates combinations of these two parameters (3x3 = 9 possible combinations), build models with these parameter combinations, evaluate the performance on **validation** dataset and choose the one that perform the best.\n",
    "\n",
    "In grid search, a common validation strategy is k-fold cross validation. In k-fold cross-validation, the training dataset is randomly partitioned into `k` equal size partitions. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation, which we will use to choose the best model.\n",
    "\n",
    "In `sklearn`, grid search + k-fold validation is implemented in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a GridSearchCV, we first have to determine the hyperparameters and possible values of parameters that we want to use. Each class of predictive models will have different kind of parameters (e.g. decision tree will be different to a regression). For this tutorial, we will search on 3 hyperparameters:\n",
    "\n",
    "1. Criterion: The function to measure the quality of a split. There are two criterias we will use, “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "2. Max depth: The maximum depth of the tree. Let's start with range of 2-10.\n",
    "3. Min samples leaf: The minimum number of samples required to be at a leaf node, allowing us to limit the minimum size of a leaf node. Let's start with range of 20-50 with step of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.602712282324\n",
      "Test accuracy: 0.552705661558\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.64      0.59      1599\n",
      "          1       0.56      0.46      0.51      1598\n",
      "\n",
      "avg / total       0.55      0.55      0.55      3197\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 40}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(3, 10),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test `accuracy` of our model is around the same with the previous best. See if we can further optimise on these parameters. Let's do another grid search, now being specific around the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.589459084605\n",
      "Test accuracy: 0.556146387238\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.74      0.63      1599\n",
      "          1       0.59      0.37      0.45      1598\n",
      "\n",
      "avg / total       0.57      0.56      0.54      3197\n",
      "\n",
      "{'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 40}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV #2\n",
    "params = {'criterion': ['gini'],\n",
    "          'max_depth': range(2, 5),\n",
    "          'min_samples_leaf': range(40, 61, 5)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accuracy` is improved over the previous best, which means our parameter tuning is successful. The model ended up being really simple, being only 2 levels deep. Try to experiment with other kinds of parameters and see whether you could do better than this. You can find the list of parameters available in a decision tree in `sklearn`'s `DecisionTreeClassifier` documentation website.\n",
    "\n",
    "## Tips\n",
    "\n",
    "1. Always perform feature importance and visualization to understand your decision tree. The best model can be found in `cv.best_estimator_`.\n",
    "2. We will use feature importance a lot across this decision tree modelling process. Rather than writing the script multiple times (which is tedious and error-prone), we can just wrap them in functions in `dm_tools.py` and import it from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inside `dm_tools.py' together with data_prep()\n",
    "import numpy as np\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', importances[i])\n",
    "\n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftAvgLast : 0.387045194994\n",
      "DemMedHomeValue : 0.166324407965\n",
      "GiftTimeLast : 0.134385809262\n",
      "GiftAvgCard36 : 0.132119592627\n",
      "PromCnt12 : 0.0362176214752\n",
      "GiftCntAll : 0.0352183147274\n",
      "GiftCnt36 : 0.0348290859586\n",
      "GiftCntCardAll : 0.0261042600701\n",
      "DemMedIncome : 0.0257531126317\n",
      "PromCntCard36 : 0.0220026002895\n",
      "DemCluster_12 : 0.0\n",
      "StatusCat96NK_S : 0.0\n",
      "DemCluster_0 : 0.0\n",
      "DemCluster_1 : 0.0\n",
      "DemCluster_10 : 0.0\n",
      "DemCluster_11 : 0.0\n",
      "DemCluster_15 : 0.0\n",
      "DemCluster_13 : 0.0\n",
      "DemCluster_14 : 0.0\n",
      "StatusCat96NK_L : 0.0\n"
     ]
    }
   ],
   "source": [
    "# do the feature importance and visualization analysis on GridSearchCV's best model\n",
    "from dm_tools import analyse_feature_importance, visualize_decision_tree\n",
    "\n",
    "analyse_feature_importance(cv.best_estimator_, X.columns, 20)\n",
    "visualize_decision_tree(cv.best_estimator_, X.columns, \"dm_best_cv.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GridSearchCV decision tree](http://dataminingtuts.s3.amazonaws.com/dm_best_cv.png)\n",
    "\n",
    "## End notes and next practical\n",
    "\n",
    "This practical introduces the concepts and techniques of data partitioning. Using the training/test data, we built decision trees and evaluate performance. We analysed feature importance and visualised the decision tree structure. Upon discovering overfitting, we optimised the hyperparameters of the decision trees using GridSearchCV and managed to improve its performance on test dataset.\n",
    "\n",
    "Next practical, we will focus on performing predictive classification modelling using logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
